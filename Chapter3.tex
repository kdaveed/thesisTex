\chapter{Problem Statement}


This chapter is divided into three sections. As the application is highly dependent on the underlying data model, the first section is dedicated to the data scheme describing the investigations in scope. The second chapter in turn addresses the problem of web applications that allows the creation of RDF data explained in the first section. It covers the issues of both the client and server side implementation and their communication. Finally section \ref{33} outlines the scheme of the solution proposed and implemented by the thesis work.


\section{Modeling anthropological research activity} \label{ps1}


This section consist of three subsections. The first two (\ref{331} and \ref{332}) describes how the RDFBones ontology (developed during the project) integrates the \textit{FMA} and \textit{OBI} ontologies for describing research processes related to anthropology. While the third section (\ref{333}) discusses how can the core ontology be extended to define custom bone segments and processes.


\subsection{Data on skeletal remains} \label{331}


We have seen in section \ref{fma} the base structure of the human skeleton. The most important point is that not only individual bones will be represented in the data we create, but the skeletal regions as well, like skull or vertebral column. The institute where these investigations are conducted posses mainly  skeletal remains of skulls. The skull has the peculiarity that it does not consists directly of bones organs, but from two sub skeletal divisions, and these two subdivisions contain the bone organs. \figref{tripleSkull} shows the ontology subset for the skull and the data instances (each denoted with \#). The red arrows denotes restrictions on the properties \textit{fma:systemic\_part\_of}.  

\img{\III}{rdfSkull.png}{Ontology and triples of the skull}{tripleSkull} 

So we know how skull and its subdivisions and bone organs are represented by RDF, but there are processes where specific bone segments have to be addressed as well. Therefor RDFBones have the class \textit{rdfbones:SegmentOfSkeletalElement}. This instances of this class is connected to the instances of the class \textit{Bone Organ} with the property \textit{fma:regional\_part\_of}.

\img{\III}{rdfBonesCard.png}{Bone segment in RDFBones}{rdfBonesCard} 

Furthermore these instances representing skeletal remains do not stay in the database individually. If a researcher takes a specific skull from the collection of the institute, it makes a so-called skeletal inventory, which records what bones segments are complete, partly present or missing. To store these information in the RDFBones ontology introduces three further classes (above the one for bone segment), which are all the subclasses of \textit{OBI} classes. 


\img{\III}{core.png}{RDFBones as extension of OBI}{rdfBonesCard} 

\figref{case1} illustrates the dataset through a simple problem. The upper left part of the figure shows that a specific bone is divided into three bone segments, while the right shows the existing bone from which the data has to be stored. It can be seen the section I. is complete, section II. is just partly present and the III. is missing. The lower part of the figure then shows that the three segment are represented as subclass of the \textit{:SegmentOfSkeletalElement} class (denoted with blue), and from the third type there are no instance have been created, while the first two are connected to the skeletal inventory and compleness label instances through \textit{:Completeness2States} instances. 

\img{\III}{case1.png}{Custom bone segment example}{case1} 


This is the way how the information is stored about skeletal remains using the RDFBones ontology.


\subsection{Investigation process}\label{332}


An investigation is done by execution of the a study design. It has three parts, the assay, the data transformation and the drawing of a conclusion. From assays and data transformations there can be more in one execution, but there is only one conclusion. The inputs of the assays are always segments of skeletal elements, an their output is always a measurement datum, while the data transformation's  input and output are both measurement datums.

\img{\III}{rdfBonesO.png}{Applied subset of OBI ontology}{rdfBonesO} 


To understand a bit more what this data model can be actually used, let us take the example of an investigation, whose goal is to determine if a taken skull belonged to a male or female. The basis is that the male and female skeleton has different peculiarities that can be quantified, how expressed they are. \figref{glabella} illustrates an example the token \textit{Glabella}, which is on the \textit{Nasal bone}, and its expressions.  

\img{\III}{glabExpressions.png}{Glabella and its expressions}{glabella}


The larger the numbers for masculine and the lowers are feminine expression. An assay in this case is an assignment of a scalar value to a bone segment. The investigation process does not take only one bone segment but several different ones, to reduce the possibility of the erroneous output. \figref{sde} shows a dataset of a study design execution where the green arrows stand for \textit{has specified input} and the blue ones for the \textit{has specified output} predicates. The idea is simple, the output of the assays are aggregated, and if the output is smaller then zero then it was a male, otherwise a female.

\img{\III}{sde.png}{Study design execution dataset}{sde} 


Where the light grey boxes are the instances of \textit{Assay}, \textit{Data Transformation} and \textit{Drawing conclusion} classes respectively.




\subsection{Ontology Extensions} \label{333}


The previous two sections introduced the data scheme of the problems of the project. This part provides a more detailed explanation about how exactly these ontologies can be extended to tackle custom problems. By skeletal inventories \textit{RDFBones} ontology allows to define custom bone segments of the bone organs in order to enable more fine-grained representation of the research activity. However by most of the cases it is sufficient to address the bone as a whole. For such cases \textit{RDFBones} has the class \textit{:PrimarySkeletalInventory} which encompasses all the entire bone organs. The definition of this inventory can be seen on the upper part of \figref{prim}.

\img{\III}{siExt.png}{Ontology extension for skeletal inventories}{prim}

The class \textit{:EntireBoneOrgan} has as many subclass as many bone organs are there. The classes \textit{X} and \textit{Y} represents the set of all bone organs. Furthermore each of the entire bones are connected to the custom subclasses of \textit{:Completeness2States}, to establish the connection to the primary skeletal inventory. The reason why there is not only one entire bone class for all bone organs, is that the input of the assays must be a bone segment, and it must be possible to address each of them individually. The lower part of \figref{prim} shows a custom inventory definition, by new completeness and bone segment classes.


By the study design execution only the assay part will be covered, because the data transformation has exactly the same structure. As it was already mentioned through the example from \figref{sde}, each assay must address a bone segment an input, and a measurement datum as an output. To define a custom study design execution (\figref{sdeExt}) at first a subclass is needed (\text{:SexExstimation}), and the different custom assays (denoted with blue) that are connected to each other again with restrictions. Then the outputs of the assays are custom scalar measurement datum classes.

\img{\III}{extensionAssay.png}{Ontology extension for sex estimation}{sdeExt}

\section{RDF Data input}

\subsection{Dynamic data entry forms}


In section \ref{223} it was explained that the simplest data input process by RDF data is the substitution of the values coming from the client into a predefined set of RDF triples. In this case the interface is a static HTML form, the input data is a set of key-value pairs, while the RDF triples are defined on the server simply by a string. However in the previous section we have seen that the data models of the problems contain relationships with 1 to n cardinalities. This means that the data input process may contain, above a selection of the type, and setting some of the literal values of one new RDF instance, the dynamic adding of sub forms that represent further instances that are connected to the main instance with cardinality n. 


\img{\III}{332_1.png}{Multi dimensional form layout}{332_1}


\figref{332_1} depicts a form which allows to add new sub forms that represent the sud subdivisions of the selected skeletal subdivision. Such functionality can be implemented by means of JavaScript routines embedded into the HTML document. First of all the task of the program on the client is to handle the click events on the \textit{Add} buttons and add further HTML elements to the form. Furthermore JS has the responsibility too, that the appropriate data is generated upon the user actions. To achieve this, the values of the form elements has to accessed by JS and set to the data object with certain keys. Each sub form can be considered just as an static HTML forms, so their data is again a set of key-value pairs. As there multiple sub forms can be added, JS stores the sub form data objects in arrays.

\begin{lstlisting}[captionpos=b, caption=JSON object generated by the form, label=332L1,
basicstyle=\footnotesize,frame=single]
{
	subdivisionType : "fma:5018",
	subDivisionLabel : "Skull_5733FS2",
	systemic_parts : [
		{
			subsubdivisionType : "fma:45720",
			subSubdivisionLabel : "Neurocranium:_93KE43",
		}, { ... } ]
}
\end{lstlisting}



The server then in turn has to be prepared that a particular subgraph of the data model has to be created multiple times, and the values are arriving in arrays. The following images illustrates how can it be imagined.

\img{\III}{332_2.png}{Multi dimensional RDF dataset}{332_1}





\subsection{Form dependencies} \label{formDependencies}


\img{\III}{subformDependency.png}{Subform dependencies}{mdfl}

These dependencies can occur not only between subforms but as well by in form dependencies where the selector elements can change based on selections.

\subsection{Instance browsing}


\begin{itemize}
	\item Figure ~\ref{navigator3} shows a further option for instance selection. 
\end{itemize}

\img{\III}{navigator.png}{Navigator example}{navigator3}

\begin{itemize}
	\item The implementation requires on the server side query and the grouping of the result
	\item Client side - programming the navigator window
\end{itemize}


\subsection{Validation}

As by each form there is required field - required - restrictions
The client has to get the information, about restrictions, and so  


\subsection{Editing form data}
	
As it was already addressed in section \ref{vivoCef}, the dataset created by the forms have to be edited as well. By editing, the HTTP request calling the entry form contains an additional field, namely the \textit{objectUri}. Based on the data model of the form, the server has to prepare the dataset, in our case a JSON object. The challenge of the server implementation is that in such multi dimensional dataset, it is not sufficient to perform only one query for the whole form data.

\img{\III}{exampleDataModel.png}{Example data model}{exDatMod}

\begin{lstlisting}[captionpos=b, caption=SPARQL query for the form data, label={sparqlExisting},
basicstyle=\footnotesize,frame=single]
SELECT ?a ?b
WHERE {
	?objectUri		p1		?a .
	?objectUri		p2		?b .
	FILTER ( ?objectUri = <inputParameter>)
}
\end{lstlisting}

The example data model from \figref{exDatMod} helps to understand the problem in more detail. If the SPARQL query on \listref{sparqlExisting} for variables ?a and ?b with incoming \textit{objectUri} value were executed, then result table of the query is inconvenient to process. For example if there are two instances for both ?a and ?b present in the dataset, then the result table contains \textit{2 x 2 = 4} elements (\tableref{sparqlResult}).

\begin{table}

	\begin{center}
		\begin{tabular}{||c | c||} 
			\hline
			?a & ?b  \\ [0.5ex] 
			\hline\hline
			a1 & b1 \\ 
			\hline
			a1 & b2 \\
			\hline
			a2 & b1 \\
			\hline
			a2 & b2 \\ [1ex] 
			\hline 
		\end{tabular}
	\end{center}
	\caption{SPARQL Result}  \label{sparqlResult}
\end{table}


Therefore the data object of the form has to be retrieved gradually, by dividing the data model graph by the predicates, whose cardinality is larger than one.

The next step after that the server has prepared the multi dimensional JSON object for the client, is to restore the state of the form, in which it was submitted by the user. This requires firstly the filling of the fields with the existing values, and adding the sub forms based on the arrays. Secondly the options of the selectors must be loaded, so that they conform to the dependencies explained in section \ref{formDependencies}. 

Finally if a value of selector or literal field changes, or new sub forms has to be added or removed, the entry form data should not be completely sent again to the server, but only the data fields that are concerned by the modification. Thus it does not require a complete page reload, and these operation can be performed through AJAX calls. To achieve this the client has to be prepared to be able to send data modification requests to the server on change event of any form element or sub form.

\subsection{Saving data}




\section{Solution Scheme} \label{33}








