

\chapter{Conclusion}

\section{Evaluation}

The applied VIVO framework offered the possibility to define data input processes in a declarative way to some limited extent by defining the elements of the input form and the RDF graph pattern. The simplicity lied in that VIVO allowed the setting of literals of particular instances, which required only static HTML forms and simple value substitution algorithms. However in the \textit{RDFBones} project the emphasis was not on the literals but on the RDF instances, and there were such cases where multiple instances had to be created through one data entry form. This required dynamic web pages with handler routines instead of static form elements, and the server routines had to become more complex too. Important challenge moreover that rules regarding which entity belongs to which are declared in ontology extension, and these definitions had to be considered by the interface generation.

During thesis the VIVO idea were further developed, so that the system can cope with the more complex problems. To achieve that the individual cases can be solved without coding rapidly, an extended vocabulary were designed that is able to express the problems of multi level data RDF input, an the code library were developed for the client and the server that were able to manage the advanced functionality based on a certain problem descriptor dataset. In the vocabulary related to the data definition the most important advancement is that it is possible to express if a the subject and the object of a particular triple in the graph pattern are in one-to-one or in one-to-many relationship with each other (\textit{Triple} vs. \textit{MultiTriple}). On the form definition this cardinality related definition is reflected by the sub forms (\textit{SubformAdder}). These elements allowed the expression of forms and data processor routines that can handle multi dimensional dataset. Further improvement that the vocabulary for data definition does not relate only to the RDF triples that were supposed to be stored, but to RDF triples as well which contained description of the system, namely the OWL restriction in the ontology extensions (\textit{RestrictionTriple}). The utility of this definition that it connects RDF nodes that were represented on the interface, thus the dependency between form elements could be expressed as well in a declarative way. From the restriction triples the appropriate SPARQL queries are generated, and the client and server algorithm together through AJAX calls could realize the adaptive interface. Finally, we have seen that the resulting RDF dataset does not necessarily consists only of new instances, but existing ones as well. The developed vocabulary is able to express these cases, and make possible the convenient browsing and selection of them with further form elements and constraints (\textit{AuxNodeSelector}, \textit{InstanceSelector}. \textit{InstanceRestrictionTriple}). 

The main benefit of the system that it abstracts from the low level implementation details, and the developer does not have to care about how the data created, edited and deleted by the application. It is sufficient to think about the scheme, the constraints and the mapping to the interface of the data describing a particular entity, and the generic client and server libraries realize the data flow between the user and database. The system can be applied then for any kind of problems where the rules of the system lies in ontological statements. Moreover due to the dynamic VIVO profile page, which allows the discovery of the RDF data graph, the created instances can be browsed without additional programming, and their literal values can be set through custom entry forms developed with the original VIVO framework tools. So the implemented system embedded into VIVO offers a widely employable Semantic Web based data management application.

\section{Future work}

\subsubsection{Improved handling of OWL restrictions}

Currently, the implemented framework cannot handle scalar measurement data, only categorical data, which is represented by RDF instances in the extensions. But there are such subprocesses of the investigations where the output data is some scalar value or even a string. In this case, the restriction points to some class of the XML schema representing a literal type. This means regarding the user interface, that the type of the particular data input fields is not given directly in the form descriptor, but it was retrieved from the ontology extension. The handling of such cases would lead to more even more adaptive data input forms.

The data input process descriptor vocabulary allows the handling of different types of restrictions, like \textit{owl:allValuesFrom}, \textit{owl:someValuesFrom} and qualified cardinality restrictions. However, currently all of them are considered just as connectors between classes, not as exact data rule descriptors. Thus the expressiveness of the OWL vocabulary was exploited only to a very limited extent. In the future, the server should send not only the list of the classes as options of the particular selectors but as well how many of the individual pieces have to be added at least or most from them. Then form handler routines could ensure that only such dataset is created that fulfills the cardinality constraints as well.

\subsubsection{RDF data based form configuration}

Currently the data input problem descriptor vocabulary is implemented in Java classes, and thus the descriptor data is in the form of Java objects. Therefore application programming still needs the creation Java files, and writing some tiny Java code for the object instantiation. In the future the vocabulary will be implemented as an OWL ontology. This solution would mean the advantage that it would be possible to build RDF data input forms completely the same way just for the descriptor RDF data input. Thus the implemented framework could be programmable completely through the interface, since VIVO profile pages for data display can be configured as well through RDF triples.

