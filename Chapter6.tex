

\chapter{Conclusion}

\section{Evaluation}

The applied VIVO framework offered the possibility to define data input processes in a declarative way to some limited extent by defining the elements of the input form and the RDF graph pattern. The simplicity lied in that VIVO allowed the setting of literals of particular instances, which required only static HTML forms and simple value substitution algorithms. However in the \textit{RDFBones} project the emphasis was not on the literals but on the RDF instances, and there were such cases where multiple instances had to be created through one data entry form. This required dynamic web pages with handler routines instead of static form elements, and the server routines had to become more complex too. Important challenge moreover that rules regarding which entity belongs to which are declared in ontology extension, and these definitions had to be considered by the interface generation.

During thesis the VIVO idea were further developed, so that the system can cope with the more complex problems. To achieve that the individual cases can be solved without coding rapidly, an extended vocabulary were designed that is able to express the problems of multi level data RDF input, an the code library were developed for the client and the server that were able to manage the advanced functionality based on a certain problem descriptor dataset. In the vocabulary related to the data definition the most important advancement is that it is possible to express if a the subject and the object of a particular triple in the graph pattern are in one-to-one or in one-to-many relationship with each other (\textit{Triple} vs. \textit{MultiTriple}). On the form definition this cardinality related definition is reflected by the sub forms (\textit{SubformAdder}). These elements allowed the expression of forms and data processor routines that can handle multi dimensional dataset. Further improvement that the vocabulary for data definition does not relate only to the RDF triples that were supposed to be stored, but to RDF triples as well which contained description of the system, namely the OWL restriction in the ontology extensions (\textit{RestrictionTriple}). The utility of this definition that it connects RDF nodes that were represented on the interface, thus the dependency between form elements could be expressed as well in a declarative way. From the restriction triples the appropriate SPARQL queries are generated, and the client and server algorithm together through AJAX calls could realize the adaptive interface. Finally, we have seen that the resulting RDF dataset does not necessarily consists only of new instances, but existing ones as well. The developed vocabulary is able to express these cases, and make possible the convenient browsing and selection of them with further form elements and constraints (\textit{AuxNodeSelector}, \textit{InstanceSelector}. \textit{InstanceRestrictionTriple}). 

The main benefit of the system that it abstracts from the low level implementation details, and the developer does not have to care about how the data created, edited and deleted by the application. It is sufficient to think about the scheme, the constraints and the mapping to the interface of the data describing a particular entity, and the generic client and server libraries realize the data flow between the user and database. The system can be applied then for any kind of problems where the rules of the system lies in ontological statements. Moreover due to the dynamic VIVO profile page, which allows the discovery of the RDF data graph, the created instances can be browsed without additional programming, and their literal values can be set through custom entry forms developed with the original VIVO framework tools. So the implemented system embedded into VIVO offers a widely employable Semantic Web based data management application.

\section{Future work}

\subsubsection{Improved handling of OWL restrictions}

Currently the implemented framework cannot handle scalar measurement data. Only categorical data, which is represented by RDF instances in the extensions. But there are such sub processes of the investigation part of where the output data is some scalar value or even a string, where some textual description has to be entered by the user. Such case would means in terms of the user interface description, that the type of the particular data field is not given directly by the descriptor dataset, but it were retrieved from the ontology extension. This feature would allow more flexible definition of the data input forms.

The data input process descriptor vocabulary allows the handling of different types of restrictions, like \textit{owl:allValuesFrom}, \textit{owl:someValuesFrom} and qualified cardinality restrictions. However, currently all of them are considered just as connectors between classes, not as exact data rules descriptors. So the expressiveness of the OWL vocabulary were exploited only to a very limited extent. In the future the server have to send not only the list of the classes as options of the particular selector, but as well how many of the individual pieces have to be added at least. Then based on the extended data the JavaScript could ensure that only such dataset is created that fulfills the cardinality constraints as well.

\subsubsection{RDF data based form configuration}

Currently the data input problem descriptor vocabulary is implemented in Java, and thus the descriptor data is Java objects. Therefore the programming of the application still need creating Java files, and writing some Java code for the object instantiation. In the future the vocabulary will be implemented by an OWL ontology, which would allow the the form and data configuration instances were in an RDF dataset. This solution would mean the advantage that it would be possible build RDF data input forms completely the same way as it is done for the RDFBones project datasets, for the descriptor RDF data input. Therefore the implemented framework could be programmable completely through the interface since VIVO profile pages for data input can be configured as well through RDF triples.







